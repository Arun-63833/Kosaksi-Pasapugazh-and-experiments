{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8538994,"sourceType":"datasetVersion","datasetId":5100933}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install ultralytics\nfrom ultralytics import YOLO\nfrom IPython.display import clear_output\nclear_output()\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T15:06:51.143021Z","iopub.execute_input":"2024-05-28T15:06:51.143382Z","iopub.status.idle":"2024-05-28T15:07:17.421313Z","shell.execute_reply.started":"2024-05-28T15:06:51.143350Z","shell.execute_reply":"2024-05-28T15:07:17.420334Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\nmodel = YOLO(\"yolov8n.yaml\")\nfor name, param in model.named_parameters():\n    if 'model.model.22.cv3.2.1.conv.weight' in name or 'model.model.22.cv3.2.1.bn.weight' in name or 'model.model.22.cv3.2.1.bn.bias' in name or 'model.model.22.cv3.2.2.weight' in name or 'model.model.22.cv3.2.2.bias' in name:\n        param.requires_grad = True  # Freeze the parameters\n    else:\n        param.requires_grad = False\nr=model.train(data=\"/kaggle/input/1000-catv-dog/data.yaml\", epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T12:13:12.182212Z","iopub.execute_input":"2024-05-28T12:13:12.183348Z","iopub.status.idle":"2024-05-28T13:36:36.477824Z","shell.execute_reply.started":"2024-05-28T12:13:12.183300Z","shell.execute_reply":"2024-05-28T13:36:36.476858Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.23 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/kaggle/input/1000-catv-dog/data.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 755k/755k [00:00<00:00, 25.5MB/s]\n2024-05-28 12:13:15,290\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n2024-05-28 12:13:16,070\tINFO util.py:124 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \nYOLOv8n summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240528_121543-nlcbuat4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arunnithyaanandam/YOLOv8/runs/nlcbuat4' target=\"_blank\">train</a></strong> to <a href='https://wandb.ai/arunnithyaanandam/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arunnithyaanandam/YOLOv8' target=\"_blank\">https://wandb.ai/arunnithyaanandam/YOLOv8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arunnithyaanandam/YOLOv8/runs/nlcbuat4' target=\"_blank\">https://wandb.ai/arunnithyaanandam/YOLOv8/runs/nlcbuat4</a>"},"metadata":{}},{"name":"stdout","text":"Freezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/1000-catv-dog/train/labels... 3792 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3792/3792 [00:09<00:00, 379.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/1000-catv-dog/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/1000-catv-dog/valid/labels... 361 images, 0 backgrounds, 0 corrupt: 100%|██████████| 361/361 [00:01<00:00, 298.77it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/1000-catv-dog/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      2.35G      2.737      3.255       3.47        111        640: 100%|██████████| 237/237 [00:49<00:00,  4.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:04<00:00,  2.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.262      0.378      0.207       0.11\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      2/100      2.32G      1.491      2.164      2.056        141        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.425      0.619      0.395      0.273\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      3/100       2.3G      1.213      1.866      1.757        110        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.36it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.413      0.637      0.392      0.298\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      4/100       2.3G      1.062      1.717      1.604        151        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.41      0.695      0.416      0.312\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      5/100      2.27G     0.9979      1.622      1.538        110        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:03<00:00,  3.93it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.436      0.725      0.429      0.333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      6/100       2.3G     0.9355      1.558      1.476         88        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.444      0.722      0.439      0.346\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      7/100       2.3G     0.8919      1.521      1.444        105        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.453      0.735      0.449      0.369\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      8/100       2.3G     0.8349      1.464      1.391        139        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.462      0.731      0.459      0.358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      9/100       2.3G     0.8255      1.448      1.378        118        640: 100%|██████████| 237/237 [00:45<00:00,  5.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.468      0.718      0.466      0.382\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     10/100      2.27G     0.7959       1.41       1.35        135        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.464      0.747      0.479      0.403\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     11/100       2.3G     0.7548      1.381      1.322         88        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.58it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.469      0.724      0.491      0.411\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     12/100      2.16G     0.7508      1.368      1.312         98        640: 100%|██████████| 237/237 [00:44<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.474      0.736      0.528      0.439\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     13/100       2.3G     0.7338      1.348      1.297        103        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.491       0.73      0.552      0.464\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     14/100      2.29G     0.7189      1.324      1.284        119        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.448      0.782      0.507      0.418\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     15/100      2.17G      0.715      1.329      1.282        127        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.55it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.468      0.735      0.544      0.452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     16/100      2.16G     0.7091      1.317      1.274        113        640: 100%|██████████| 237/237 [00:43<00:00,  5.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.467      0.784      0.542      0.462\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     17/100       2.3G     0.6822      1.287      1.254        111        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.522      0.699      0.584      0.494\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     18/100      2.17G       0.67      1.258      1.242        130        640: 100%|██████████| 237/237 [00:44<00:00,  5.38it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.499      0.725      0.596      0.515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     19/100       2.3G       0.67      1.256      1.238         97        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.492      0.732      0.594      0.512\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     20/100      2.27G     0.6467      1.223      1.214        106        640: 100%|██████████| 237/237 [00:44<00:00,  5.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.533      0.696      0.622      0.526\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     21/100      2.28G     0.6468      1.227      1.219        117        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.524       0.71      0.609      0.514\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     22/100      2.27G     0.6429      1.206      1.219        122        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.54      0.713      0.633      0.529\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     23/100      2.16G     0.6334      1.197      1.204        134        640: 100%|██████████| 237/237 [00:44<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.524      0.726      0.638      0.547\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     24/100      2.16G     0.6264      1.178      1.196        101        640: 100%|██████████| 237/237 [00:45<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.57      0.693      0.647      0.553\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     25/100       2.3G      0.622      1.167      1.197        124        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.545       0.72      0.653      0.569\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     26/100      2.17G     0.6073      1.142      1.178         98        640: 100%|██████████| 237/237 [00:43<00:00,  5.41it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.565        0.7      0.662      0.569\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     27/100      2.16G     0.5989      1.123      1.173        128        640: 100%|██████████| 237/237 [00:44<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.579      0.705      0.683      0.594\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     28/100       2.3G     0.5964      1.126      1.169        137        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.57      0.738      0.693      0.612\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     29/100      2.18G     0.5948      1.107      1.175        114        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.636      0.707      0.703      0.615\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     30/100      2.17G     0.5911      1.101      1.169        125        640: 100%|██████████| 237/237 [00:44<00:00,  5.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:03<00:00,  3.06it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.647      0.703       0.73      0.631\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     31/100      2.27G     0.5814      1.081      1.157        103        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.59it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.673      0.685      0.729       0.63\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     32/100      2.16G     0.5898      1.082      1.162        110        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.627      0.744      0.739       0.65\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     33/100       2.3G     0.5778       1.05      1.153        112        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.674      0.736      0.762      0.666\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     34/100      2.17G     0.5674      1.042      1.144         86        640: 100%|██████████| 237/237 [00:44<00:00,  5.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.66      0.748       0.76      0.664\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     35/100       2.3G       0.56      1.016      1.141        129        640: 100%|██████████| 237/237 [00:44<00:00,  5.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.688      0.737      0.775      0.684\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     36/100      2.27G     0.5565      1.016      1.136         98        640: 100%|██████████| 237/237 [00:45<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.701      0.721      0.776      0.688\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     37/100      2.17G     0.5593          1      1.136        116        640: 100%|██████████| 237/237 [00:44<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.67it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.711      0.727      0.786      0.692\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     38/100      2.18G     0.5579     0.9925      1.134        124        640: 100%|██████████| 237/237 [00:45<00:00,  5.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.716      0.718       0.79        0.7\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     39/100      2.18G     0.5511     0.9755      1.127        109        640: 100%|██████████| 237/237 [00:44<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.713      0.742      0.792      0.705\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     40/100      2.29G     0.5514     0.9681      1.129        123        640: 100%|██████████| 237/237 [00:44<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.767       0.75      0.821       0.73\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     41/100       2.3G     0.5433     0.9513      1.124        105        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.704      0.705      0.762      0.679\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     42/100      2.27G     0.5445     0.9479      1.121        123        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.742      0.761      0.817      0.735\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     43/100      2.16G     0.5405     0.9378       1.12        105        640: 100%|██████████| 237/237 [00:44<00:00,  5.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.76      0.737       0.82      0.733\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     44/100      2.19G     0.5338     0.9286      1.113        137        640: 100%|██████████| 237/237 [00:45<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.744      0.761      0.822      0.732\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     45/100       2.3G     0.5251     0.9022      1.106        119        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.755      0.777      0.834       0.75\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     46/100      2.17G     0.5246     0.8943      1.107        116        640: 100%|██████████| 237/237 [00:45<00:00,  5.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.74      0.807      0.845      0.763\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     47/100      2.27G     0.5191     0.8954      1.102        126        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.762      0.772      0.839      0.751\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     48/100       2.3G     0.5208      0.875      1.098        115        640: 100%|██████████| 237/237 [00:45<00:00,  5.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.776      0.818      0.864       0.78\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     49/100      2.17G     0.5147      0.864      1.097        124        640: 100%|██████████| 237/237 [00:44<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.79      0.804      0.867      0.785\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     50/100      2.16G     0.5116     0.8476      1.092        121        640: 100%|██████████| 237/237 [00:45<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.817      0.781      0.865      0.779\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     51/100      2.27G     0.4993     0.8331      1.084        148        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.783      0.798      0.866      0.788\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     52/100       2.3G     0.5065     0.8373      1.088        107        640: 100%|██████████| 237/237 [00:44<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.84      0.781      0.878        0.8\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     53/100      2.27G     0.5066     0.8199      1.088        125        640: 100%|██████████| 237/237 [00:44<00:00,  5.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.833      0.809      0.884      0.803\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     54/100      2.16G     0.4957      0.816      1.081        134        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.84      0.787      0.885      0.808\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     55/100      2.29G     0.4915     0.7993      1.079        100        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.853      0.813      0.892      0.812\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     56/100      2.17G     0.4883     0.7995      1.075         97        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.811      0.818      0.887      0.812\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     57/100       2.3G     0.4921     0.7906      1.074        117        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.841      0.814      0.894      0.818\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     58/100      2.16G     0.4866     0.7805      1.071        129        640: 100%|██████████| 237/237 [00:44<00:00,  5.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.835      0.823      0.892      0.811\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     59/100       2.3G     0.4842     0.7782      1.068         94        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.825       0.83      0.893      0.819\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     60/100      2.29G      0.478     0.7672      1.066        116        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.77it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.842      0.828        0.9      0.827\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     61/100       2.3G     0.4814     0.7692      1.067        129        640: 100%|██████████| 237/237 [00:44<00:00,  5.29it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.837      0.823      0.895      0.821\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     62/100      2.17G     0.4791     0.7571      1.063        139        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.855      0.828        0.9      0.827\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     63/100      2.17G     0.4753     0.7451       1.06         99        640: 100%|██████████| 237/237 [00:44<00:00,  5.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.86       0.82      0.906      0.832\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     64/100      2.16G      0.475     0.7379      1.062        119        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.844      0.848      0.906      0.838\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     65/100       2.3G     0.4683     0.7202      1.053        125        640: 100%|██████████| 237/237 [00:44<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.877      0.824      0.906      0.831\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     66/100      2.27G     0.4634     0.7233      1.053         97        640: 100%|██████████| 237/237 [00:44<00:00,  5.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.852       0.85      0.907      0.835\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     67/100      2.27G     0.4675     0.7218      1.053        124        640: 100%|██████████| 237/237 [00:44<00:00,  5.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.891      0.825      0.917      0.849\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     68/100      2.16G     0.4579     0.7109      1.046         99        640: 100%|██████████| 237/237 [00:44<00:00,  5.33it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.70it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.892      0.847      0.919      0.849\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     69/100      2.27G     0.4547      0.696      1.044        141        640: 100%|██████████| 237/237 [00:45<00:00,  5.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.74it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.865       0.85      0.918       0.85\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     70/100      2.29G     0.4537     0.6911      1.045        123        640: 100%|██████████| 237/237 [00:44<00:00,  5.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.69it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.858      0.861       0.92      0.856\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     71/100      2.29G     0.4549     0.6854      1.044        154        640: 100%|██████████| 237/237 [00:44<00:00,  5.30it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.888      0.842      0.918      0.852\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     72/100      2.27G      0.446     0.6794      1.035        126        640: 100%|██████████| 237/237 [00:44<00:00,  5.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.861      0.855      0.918       0.85\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     73/100      2.29G     0.4502     0.6708      1.036        116        640: 100%|██████████| 237/237 [00:44<00:00,  5.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.881      0.867      0.925      0.859\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     74/100      2.27G     0.4472     0.6689      1.039        128        640: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.64it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.909      0.827      0.921      0.856\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     75/100      2.27G     0.4455      0.665      1.036        130        640: 100%|██████████| 237/237 [00:44<00:00,  5.38it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.891      0.863      0.927      0.864\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     76/100      2.27G     0.4392     0.6597      1.035         97        640: 100%|██████████| 237/237 [00:42<00:00,  5.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.89it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.886      0.866      0.929      0.867\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     77/100       2.3G     0.4366     0.6599      1.026        100        640: 100%|██████████| 237/237 [00:43<00:00,  5.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.883      0.872       0.93      0.866\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     78/100      2.29G     0.4343     0.6523      1.027        114        640: 100%|██████████| 237/237 [00:42<00:00,  5.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.888      0.862      0.928      0.864\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     79/100       2.3G     0.4394     0.6476      1.029         86        640: 100%|██████████| 237/237 [00:43<00:00,  5.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.877      0.857      0.924      0.859\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     80/100      2.17G     0.4313     0.6311      1.023        113        640: 100%|██████████| 237/237 [00:42<00:00,  5.53it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.894      0.867      0.926      0.862\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     81/100      2.29G     0.4317      0.638      1.026        132        640: 100%|██████████| 237/237 [00:43<00:00,  5.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.908      0.861      0.929      0.866\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     82/100      2.27G     0.4225     0.6173      1.016        152        640: 100%|██████████| 237/237 [00:42<00:00,  5.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.84it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.894      0.863      0.929      0.865\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     83/100      2.29G     0.4212     0.6189      1.019        113        640: 100%|██████████| 237/237 [00:43<00:00,  5.45it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.76it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.903      0.861      0.929      0.869\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     84/100      2.29G     0.4246     0.6323      1.023        145        640: 100%|██████████| 237/237 [00:42<00:00,  5.52it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.905      0.861       0.93       0.87\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     85/100      2.27G     0.4182     0.6175      1.017        118        640: 100%|██████████| 237/237 [00:42<00:00,  5.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.896      0.879      0.935      0.873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     86/100      2.29G     0.4189     0.6087      1.018        101        640: 100%|██████████| 237/237 [00:43<00:00,  5.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.899      0.883      0.933      0.872\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     87/100       2.3G     0.4143      0.605      1.011         87        640: 100%|██████████| 237/237 [00:42<00:00,  5.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.923      0.861      0.935      0.875\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     88/100      2.16G     0.4143     0.6024      1.013        138        640: 100%|██████████| 237/237 [00:43<00:00,  5.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  5.05it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.907      0.871      0.936      0.877\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     89/100      2.17G     0.4172     0.6003      1.015        131        640: 100%|██████████| 237/237 [00:42<00:00,  5.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.912      0.878      0.939      0.879\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     90/100      2.17G     0.4086     0.5952      1.007        135        640: 100%|██████████| 237/237 [00:43<00:00,  5.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.908       0.88      0.936      0.879\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     91/100      2.29G     0.3496     0.4196     0.9556         65        640: 100%|██████████| 237/237 [00:43<00:00,  5.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.98it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.917      0.866      0.932      0.873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     92/100      2.25G     0.3376      0.389     0.9473         65        640: 100%|██████████| 237/237 [00:40<00:00,  5.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.80it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501        0.9      0.882      0.935      0.877\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     93/100      2.27G     0.3356      0.383     0.9467         50        640: 100%|██████████| 237/237 [00:40<00:00,  5.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.911       0.88      0.938      0.882\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     94/100      2.25G     0.3328     0.3812     0.9461         70        640: 100%|██████████| 237/237 [00:40<00:00,  5.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.66it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.918      0.884       0.94      0.884\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     95/100      2.27G     0.3306      0.372     0.9437         66        640: 100%|██████████| 237/237 [00:40<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.97it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.907      0.894      0.937      0.883\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     96/100      2.25G     0.3266      0.363     0.9412         71        640: 100%|██████████| 237/237 [00:40<00:00,  5.90it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501       0.92      0.884       0.94      0.886\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     97/100      2.27G     0.3213     0.3575     0.9364         51        640: 100%|██████████| 237/237 [00:40<00:00,  5.87it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.86it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.908      0.888      0.938      0.885\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     98/100      2.27G     0.3228     0.3546     0.9393         64        640: 100%|██████████| 237/237 [00:40<00:00,  5.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.79it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.903      0.889      0.939      0.886\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     99/100      2.27G     0.3205     0.3523     0.9333         66        640: 100%|██████████| 237/237 [00:40<00:00,  5.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.909      0.894       0.94      0.888\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"    100/100      2.25G     0.3184      0.345     0.9343         64        640: 100%|██████████| 237/237 [00:40<00:00,  5.85it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:02<00:00,  4.87it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.919      0.885       0.94      0.888\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n100 epochs completed in 1.331 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics YOLOv8.2.23 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\nYOLOv8n summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [00:05<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        361       1501      0.909      0.894       0.94      0.888\n                   Cat        361        831      0.909       0.88      0.934      0.878\n                   Dog        361        670      0.908      0.907      0.945      0.898\nSpeed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='13.185 MB of 13.185 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>lr/pg1</td><td>▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>lr/pg2</td><td>▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▆▇▇▇▇▇██████████████████</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>metrics/precision(B)</td><td>▁▃▃▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇▇███████████</td></tr><tr><td>metrics/recall(B)</td><td>▁▅▆▆▆▆▇▆▆▆▅▆▅▆▆▆▅▆▇▇▇▇▇▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/cls_loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/box_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/cls_loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/dfl_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>3e-05</td></tr><tr><td>lr/pg1</td><td>3e-05</td></tr><tr><td>lr/pg2</td><td>3e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.93974</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.88792</td></tr><tr><td>metrics/precision(B)</td><td>0.90887</td></tr><tr><td>metrics/recall(B)</td><td>0.89388</td></tr><tr><td>model/GFLOPs</td><td>8.195</td></tr><tr><td>model/parameters</td><td>3011238</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>3.502</td></tr><tr><td>train/box_loss</td><td>0.31838</td></tr><tr><td>train/cls_loss</td><td>0.34504</td></tr><tr><td>train/dfl_loss</td><td>0.9343</td></tr><tr><td>val/box_loss</td><td>0.32043</td></tr><tr><td>val/cls_loss</td><td>0.42068</td></tr><tr><td>val/dfl_loss</td><td>0.85324</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">train</strong> at: <a href='https://wandb.ai/arunnithyaanandam/YOLOv8/runs/nlcbuat4' target=\"_blank\">https://wandb.ai/arunnithyaanandam/YOLOv8/runs/nlcbuat4</a><br/> View project at: <a href='https://wandb.ai/arunnithyaanandam/YOLOv8' target=\"_blank\">https://wandb.ai/arunnithyaanandam/YOLOv8</a><br/>Synced 5 W&B file(s), 24 media file(s), 5 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240528_121543-nlcbuat4/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"model = YOLO('yolov8.yaml') \nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:42:11.479150Z","iopub.execute_input":"2024-05-28T15:42:11.479811Z","iopub.status.idle":"2024-05-28T15:42:11.609689Z","shell.execute_reply.started":"2024-05-28T15:42:11.479779Z","shell.execute_reply":"2024-05-28T15:42:11.608692Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"WARNING ⚠️ no model scale passed. Assuming scale='n'.\nYOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-1): 2 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:42:58.717806Z","iopub.execute_input":"2024-05-28T15:42:58.718516Z","iopub.status.idle":"2024-05-28T15:42:58.725203Z","shell.execute_reply.started":"2024-05-28T15:42:58.718486Z","shell.execute_reply":"2024-05-28T15:42:58.724369Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"model.model.0.conv.weight\nmodel.model.0.bn.weight\nmodel.model.0.bn.bias\nmodel.model.1.conv.weight\nmodel.model.1.bn.weight\nmodel.model.1.bn.bias\nmodel.model.2.cv1.conv.weight\nmodel.model.2.cv1.bn.weight\nmodel.model.2.cv1.bn.bias\nmodel.model.2.cv2.conv.weight\nmodel.model.2.cv2.bn.weight\nmodel.model.2.cv2.bn.bias\nmodel.model.2.m.0.cv1.conv.weight\nmodel.model.2.m.0.cv1.bn.weight\nmodel.model.2.m.0.cv1.bn.bias\nmodel.model.2.m.0.cv2.conv.weight\nmodel.model.2.m.0.cv2.bn.weight\nmodel.model.2.m.0.cv2.bn.bias\nmodel.model.3.conv.weight\nmodel.model.3.bn.weight\nmodel.model.3.bn.bias\nmodel.model.4.cv1.conv.weight\nmodel.model.4.cv1.bn.weight\nmodel.model.4.cv1.bn.bias\nmodel.model.4.cv2.conv.weight\nmodel.model.4.cv2.bn.weight\nmodel.model.4.cv2.bn.bias\nmodel.model.4.m.0.cv1.conv.weight\nmodel.model.4.m.0.cv1.bn.weight\nmodel.model.4.m.0.cv1.bn.bias\nmodel.model.4.m.0.cv2.conv.weight\nmodel.model.4.m.0.cv2.bn.weight\nmodel.model.4.m.0.cv2.bn.bias\nmodel.model.4.m.1.cv1.conv.weight\nmodel.model.4.m.1.cv1.bn.weight\nmodel.model.4.m.1.cv1.bn.bias\nmodel.model.4.m.1.cv2.conv.weight\nmodel.model.4.m.1.cv2.bn.weight\nmodel.model.4.m.1.cv2.bn.bias\nmodel.model.5.conv.weight\nmodel.model.5.bn.weight\nmodel.model.5.bn.bias\nmodel.model.6.cv1.conv.weight\nmodel.model.6.cv1.bn.weight\nmodel.model.6.cv1.bn.bias\nmodel.model.6.cv2.conv.weight\nmodel.model.6.cv2.bn.weight\nmodel.model.6.cv2.bn.bias\nmodel.model.6.m.0.cv1.conv.weight\nmodel.model.6.m.0.cv1.bn.weight\nmodel.model.6.m.0.cv1.bn.bias\nmodel.model.6.m.0.cv2.conv.weight\nmodel.model.6.m.0.cv2.bn.weight\nmodel.model.6.m.0.cv2.bn.bias\nmodel.model.6.m.1.cv1.conv.weight\nmodel.model.6.m.1.cv1.bn.weight\nmodel.model.6.m.1.cv1.bn.bias\nmodel.model.6.m.1.cv2.conv.weight\nmodel.model.6.m.1.cv2.bn.weight\nmodel.model.6.m.1.cv2.bn.bias\nmodel.model.7.conv.weight\nmodel.model.7.bn.weight\nmodel.model.7.bn.bias\nmodel.model.8.cv1.conv.weight\nmodel.model.8.cv1.bn.weight\nmodel.model.8.cv1.bn.bias\nmodel.model.8.cv2.conv.weight\nmodel.model.8.cv2.bn.weight\nmodel.model.8.cv2.bn.bias\nmodel.model.8.m.0.cv1.conv.weight\nmodel.model.8.m.0.cv1.bn.weight\nmodel.model.8.m.0.cv1.bn.bias\nmodel.model.8.m.0.cv2.conv.weight\nmodel.model.8.m.0.cv2.bn.weight\nmodel.model.8.m.0.cv2.bn.bias\nmodel.model.9.cv1.conv.weight\nmodel.model.9.cv1.bn.weight\nmodel.model.9.cv1.bn.bias\nmodel.model.9.cv2.conv.weight\nmodel.model.9.cv2.bn.weight\nmodel.model.9.cv2.bn.bias\nmodel.model.12.cv1.conv.weight\nmodel.model.12.cv1.bn.weight\nmodel.model.12.cv1.bn.bias\nmodel.model.12.cv2.conv.weight\nmodel.model.12.cv2.bn.weight\nmodel.model.12.cv2.bn.bias\nmodel.model.12.m.0.cv1.conv.weight\nmodel.model.12.m.0.cv1.bn.weight\nmodel.model.12.m.0.cv1.bn.bias\nmodel.model.12.m.0.cv2.conv.weight\nmodel.model.12.m.0.cv2.bn.weight\nmodel.model.12.m.0.cv2.bn.bias\nmodel.model.15.cv1.conv.weight\nmodel.model.15.cv1.bn.weight\nmodel.model.15.cv1.bn.bias\nmodel.model.15.cv2.conv.weight\nmodel.model.15.cv2.bn.weight\nmodel.model.15.cv2.bn.bias\nmodel.model.15.m.0.cv1.conv.weight\nmodel.model.15.m.0.cv1.bn.weight\nmodel.model.15.m.0.cv1.bn.bias\nmodel.model.15.m.0.cv2.conv.weight\nmodel.model.15.m.0.cv2.bn.weight\nmodel.model.15.m.0.cv2.bn.bias\nmodel.model.16.conv.weight\nmodel.model.16.bn.weight\nmodel.model.16.bn.bias\nmodel.model.18.cv1.conv.weight\nmodel.model.18.cv1.bn.weight\nmodel.model.18.cv1.bn.bias\nmodel.model.18.cv2.conv.weight\nmodel.model.18.cv2.bn.weight\nmodel.model.18.cv2.bn.bias\nmodel.model.18.m.0.cv1.conv.weight\nmodel.model.18.m.0.cv1.bn.weight\nmodel.model.18.m.0.cv1.bn.bias\nmodel.model.18.m.0.cv2.conv.weight\nmodel.model.18.m.0.cv2.bn.weight\nmodel.model.18.m.0.cv2.bn.bias\nmodel.model.19.conv.weight\nmodel.model.19.bn.weight\nmodel.model.19.bn.bias\nmodel.model.21.cv1.conv.weight\nmodel.model.21.cv1.bn.weight\nmodel.model.21.cv1.bn.bias\nmodel.model.21.cv2.conv.weight\nmodel.model.21.cv2.bn.weight\nmodel.model.21.cv2.bn.bias\nmodel.model.21.m.0.cv1.conv.weight\nmodel.model.21.m.0.cv1.bn.weight\nmodel.model.21.m.0.cv1.bn.bias\nmodel.model.21.m.0.cv2.conv.weight\nmodel.model.21.m.0.cv2.bn.weight\nmodel.model.21.m.0.cv2.bn.bias\nmodel.model.22.cv2.0.0.conv.weight\nmodel.model.22.cv2.0.0.bn.weight\nmodel.model.22.cv2.0.0.bn.bias\nmodel.model.22.cv2.0.1.conv.weight\nmodel.model.22.cv2.0.1.bn.weight\nmodel.model.22.cv2.0.1.bn.bias\nmodel.model.22.cv2.0.2.weight\nmodel.model.22.cv2.0.2.bias\nmodel.model.22.cv2.1.0.conv.weight\nmodel.model.22.cv2.1.0.bn.weight\nmodel.model.22.cv2.1.0.bn.bias\nmodel.model.22.cv2.1.1.conv.weight\nmodel.model.22.cv2.1.1.bn.weight\nmodel.model.22.cv2.1.1.bn.bias\nmodel.model.22.cv2.1.2.weight\nmodel.model.22.cv2.1.2.bias\nmodel.model.22.cv2.2.0.conv.weight\nmodel.model.22.cv2.2.0.bn.weight\nmodel.model.22.cv2.2.0.bn.bias\nmodel.model.22.cv2.2.1.conv.weight\nmodel.model.22.cv2.2.1.bn.weight\nmodel.model.22.cv2.2.1.bn.bias\nmodel.model.22.cv2.2.2.weight\nmodel.model.22.cv2.2.2.bias\nmodel.model.22.cv3.0.0.conv.weight\nmodel.model.22.cv3.0.0.bn.weight\nmodel.model.22.cv3.0.0.bn.bias\nmodel.model.22.cv3.0.1.conv.weight\nmodel.model.22.cv3.0.1.bn.weight\nmodel.model.22.cv3.0.1.bn.bias\nmodel.model.22.cv3.0.2.weight\nmodel.model.22.cv3.0.2.bias\nmodel.model.22.cv3.1.0.conv.weight\nmodel.model.22.cv3.1.0.bn.weight\nmodel.model.22.cv3.1.0.bn.bias\nmodel.model.22.cv3.1.1.conv.weight\nmodel.model.22.cv3.1.1.bn.weight\nmodel.model.22.cv3.1.1.bn.bias\nmodel.model.22.cv3.1.2.weight\nmodel.model.22.cv3.1.2.bias\nmodel.model.22.cv3.2.0.conv.weight\nmodel.model.22.cv3.2.0.bn.weight\nmodel.model.22.cv3.2.0.bn.bias\nmodel.model.22.cv3.2.1.conv.weight\nmodel.model.22.cv3.2.1.bn.weight\nmodel.model.22.cv3.2.1.bn.bias\nmodel.model.22.cv3.2.2.weight\nmodel.model.22.cv3.2.2.bias\nmodel.model.22.dfl.conv.weight\n","output_type":"stream"}]},{"cell_type":"code","source":"model=YOLO('yolov8.yaml')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:03:19.887666Z","iopub.execute_input":"2024-05-28T17:03:19.888635Z","iopub.status.idle":"2024-05-28T17:03:20.018216Z","shell.execute_reply.started":"2024-05-28T17:03:19.888599Z","shell.execute_reply":"2024-05-28T17:03:20.017253Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"WARNING ⚠️ no model scale passed. Assuming scale='n'.\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in model.named_children():\n    print(i[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-28T17:03:29.945876Z","iopub.execute_input":"2024-05-28T17:03:29.946607Z","iopub.status.idle":"2024-05-28T17:03:29.953751Z","shell.execute_reply.started":"2024-05-28T17:03:29.946575Z","shell.execute_reply":"2024-05-28T17:03:29.952845Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"DetectionModel(\n  (model): Sequential(\n    (0): Conv(\n      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (1): Conv(\n      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (2): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (3): Conv(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (4): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (5): Conv(\n      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (6): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (7): Conv(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (8): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (9): SPPF(\n      (cv1): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n    )\n    (10): Upsample(scale_factor=2.0, mode='nearest')\n    (11): Concat()\n    (12): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (13): Upsample(scale_factor=2.0, mode='nearest')\n    (14): Concat()\n    (15): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (16): Conv(\n      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (17): Concat()\n    (18): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (19): Conv(\n      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (20): Concat()\n    (21): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (22): Detect(\n      (cv2): ModuleList(\n        (0): Sequential(\n          (0): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (0): Conv(\n            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (0): Conv(\n            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (cv3): ModuleList(\n        (0): Sequential(\n          (0): Conv(\n            (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (0): Conv(\n            (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (0): Conv(\n            (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (dfl): DFL(\n        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"help('ultralytics.nn.tasks.DetectionModel')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T16:55:25.235564Z","iopub.execute_input":"2024-05-28T16:55:25.236236Z","iopub.status.idle":"2024-05-28T16:55:25.251637Z","shell.execute_reply.started":"2024-05-28T16:55:25.236204Z","shell.execute_reply":"2024-05-28T16:55:25.250713Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Help on class DetectionModel in ultralytics.nn.tasks:\n\nultralytics.nn.tasks.DetectionModel = class DetectionModel(BaseModel)\n |  ultralytics.nn.tasks.DetectionModel(cfg='yolov8n.yaml', ch=3, nc=None, verbose=True)\n |  \n |  YOLOv8 detection model.\n |  \n |  Method resolution order:\n |      DetectionModel\n |      BaseModel\n |      torch.nn.modules.module.Module\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, cfg='yolov8n.yaml', ch=3, nc=None, verbose=True)\n |      Initialize the YOLOv8 detection model with the given config and parameters.\n |  \n |  init_criterion(self)\n |      Initialize the loss criterion for the DetectionModel.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseModel:\n |  \n |  forward(self, x, *args, **kwargs)\n |      Forward pass of the model on a single scale. Wrapper for `_forward_once` method.\n |      \n |      Args:\n |          x (torch.Tensor | dict): The input image tensor or a dict including image tensor and gt labels.\n |      \n |      Returns:\n |          (torch.Tensor): The output of the network.\n |  \n |  fuse(self, verbose=True)\n |      Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the\n |      computation efficiency.\n |      \n |      Returns:\n |          (nn.Module): The fused model is returned.\n |  \n |  info(self, detailed=False, verbose=True, imgsz=640)\n |      Prints model information.\n |      \n |      Args:\n |          detailed (bool): if True, prints out detailed information about the model. Defaults to False\n |          verbose (bool): if True, prints out the model information. Defaults to False\n |          imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n |  \n |  is_fused(self, thresh=10)\n |      Check if the model has less than a certain threshold of BatchNorm layers.\n |      \n |      Args:\n |          thresh (int, optional): The threshold number of BatchNorm layers. Default is 10.\n |      \n |      Returns:\n |          (bool): True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.\n |  \n |  load(self, weights, verbose=True)\n |      Load the weights into the model.\n |      \n |      Args:\n |          weights (dict | torch.nn.Module): The pre-trained weights to be loaded.\n |          verbose (bool, optional): Whether to log the transfer progress. Defaults to True.\n |  \n |  loss(self, batch, preds=None)\n |      Compute loss.\n |      \n |      Args:\n |          batch (dict): Batch to compute loss on\n |          preds (torch.Tensor | List[torch.Tensor]): Predictions.\n |  \n |  predict(self, x, profile=False, visualize=False, augment=False, embed=None)\n |      Perform a forward pass through the network.\n |      \n |      Args:\n |          x (torch.Tensor): The input tensor to the model.\n |          profile (bool):  Print the computation time of each layer if True, defaults to False.\n |          visualize (bool): Save the feature maps of the model if True, defaults to False.\n |          augment (bool): Augment image during prediction, defaults to False.\n |          embed (list, optional): A list of feature vectors/embeddings to return.\n |      \n |      Returns:\n |          (torch.Tensor): The last output of the model.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from torch.nn.modules.module.Module:\n |  \n |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n |  \n |  __delattr__(self, name)\n |      Implement delattr(self, name).\n |  \n |  __dir__(self)\n |      Default dir() implementation.\n |  \n |  __getattr__(self, name: str) -> Any\n |      # On the return type:\n |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n |      # This is done for better interop with various type checkers for the end users.\n |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n |      # people to excessively use type-ignores, asserts, casts, etc.\n |      # See full discussion on the problems with returning `Union` here\n |      # https://github.com/microsoft/pyright/issues/4213\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n |      Implement setattr(self, name, value).\n |  \n |  __setstate__(self, state)\n |  \n |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n |      Adds a child module to the current module.\n |      \n |      The module can be accessed as an attribute using the given name.\n |      \n |      Args:\n |          name (str): name of the child module. The child module can be\n |              accessed from this module using the given name\n |          module (Module): child module to be added to the module.\n |  \n |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n |      as well as self. Typical use includes initializing the parameters of a model\n |      (see also :ref:`nn-init-doc`).\n |      \n |      Args:\n |          fn (:class:`Module` -> None): function to be applied to each submodule\n |      \n |      Returns:\n |          Module: self\n |      \n |      Example::\n |      \n |          >>> @torch.no_grad()\n |          >>> def init_weights(m):\n |          >>>     print(m)\n |          >>>     if type(m) == nn.Linear:\n |          >>>         m.weight.fill_(1.0)\n |          >>>         print(m.weight)\n |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n |          >>> net.apply(init_weights)\n |          Linear(in_features=2, out_features=2, bias=True)\n |          Parameter containing:\n |          tensor([[1., 1.],\n |                  [1., 1.]], requires_grad=True)\n |          Linear(in_features=2, out_features=2, bias=True)\n |          Parameter containing:\n |          tensor([[1., 1.],\n |                  [1., 1.]], requires_grad=True)\n |          Sequential(\n |            (0): Linear(in_features=2, out_features=2, bias=True)\n |            (1): Linear(in_features=2, out_features=2, bias=True)\n |          )\n |  \n |  bfloat16(self: ~T) -> ~T\n |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Returns:\n |          Module: self\n |  \n |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n |      Returns an iterator over module buffers.\n |      \n |      Args:\n |          recurse (bool): if True, then yields buffers of this module\n |              and all submodules. Otherwise, yields only buffers that\n |              are direct members of this module.\n |      \n |      Yields:\n |          torch.Tensor: module buffer\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> for buf in model.buffers():\n |          >>>     print(type(buf), buf.size())\n |          <class 'torch.Tensor'> (20L,)\n |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n |  \n |  children(self) -> Iterator[ForwardRef('Module')]\n |      Returns an iterator over immediate children modules.\n |      \n |      Yields:\n |          Module: a child module\n |  \n |  compile(self, *args, **kwargs)\n |      Compile this Module's forward using :func:`torch.compile`.\n |      \n |      This Module's `__call__` method is compiled and all arguments are passed as-is\n |      to :func:`torch.compile`.\n |      \n |      See :func:`torch.compile` for details on the arguments for this function.\n |  \n |  cpu(self: ~T) -> ~T\n |      Moves all model parameters and buffers to the CPU.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Returns:\n |          Module: self\n |  \n |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n |      Moves all model parameters and buffers to the GPU.\n |      \n |      This also makes associated parameters and buffers different objects. So\n |      it should be called before constructing optimizer if the module will\n |      live on GPU while being optimized.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Args:\n |          device (int, optional): if specified, all parameters will be\n |              copied to that device\n |      \n |      Returns:\n |          Module: self\n |  \n |  double(self: ~T) -> ~T\n |      Casts all floating point parameters and buffers to ``double`` datatype.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Returns:\n |          Module: self\n |  \n |  eval(self: ~T) -> ~T\n |      Sets the module in evaluation mode.\n |      \n |      This has any effect only on certain modules. See documentations of\n |      particular modules for details of their behaviors in training/evaluation\n |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n |      etc.\n |      \n |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n |      \n |      See :ref:`locally-disable-grad-doc` for a comparison between\n |      `.eval()` and several similar mechanisms that may be confused with it.\n |      \n |      Returns:\n |          Module: self\n |  \n |  extra_repr(self) -> str\n |      Set the extra representation of the module\n |      \n |      To print customized extra information, you should re-implement\n |      this method in your own modules. Both single-line and multi-line\n |      strings are acceptable.\n |  \n |  float(self: ~T) -> ~T\n |      Casts all floating point parameters and buffers to ``float`` datatype.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Returns:\n |          Module: self\n |  \n |  get_buffer(self, target: str) -> 'Tensor'\n |      Returns the buffer given by ``target`` if it exists,\n |      otherwise throws an error.\n |      \n |      See the docstring for ``get_submodule`` for a more detailed\n |      explanation of this method's functionality as well as how to\n |      correctly specify ``target``.\n |      \n |      Args:\n |          target: The fully-qualified string name of the buffer\n |              to look for. (See ``get_submodule`` for how to specify a\n |              fully-qualified string.)\n |      \n |      Returns:\n |          torch.Tensor: The buffer referenced by ``target``\n |      \n |      Raises:\n |          AttributeError: If the target string references an invalid\n |              path or resolves to something that is not a\n |              buffer\n |  \n |  get_extra_state(self) -> Any\n |      Returns any extra state to include in the module's state_dict.\n |      Implement this and a corresponding :func:`set_extra_state` for your module\n |      if you need to store extra state. This function is called when building the\n |      module's `state_dict()`.\n |      \n |      Note that extra state should be picklable to ensure working serialization\n |      of the state_dict. We only provide provide backwards compatibility guarantees\n |      for serializing Tensors; other objects may break backwards compatibility if\n |      their serialized pickled form changes.\n |      \n |      Returns:\n |          object: Any extra state to store in the module's state_dict\n |  \n |  get_parameter(self, target: str) -> 'Parameter'\n |      Returns the parameter given by ``target`` if it exists,\n |      otherwise throws an error.\n |      \n |      See the docstring for ``get_submodule`` for a more detailed\n |      explanation of this method's functionality as well as how to\n |      correctly specify ``target``.\n |      \n |      Args:\n |          target: The fully-qualified string name of the Parameter\n |              to look for. (See ``get_submodule`` for how to specify a\n |              fully-qualified string.)\n |      \n |      Returns:\n |          torch.nn.Parameter: The Parameter referenced by ``target``\n |      \n |      Raises:\n |          AttributeError: If the target string references an invalid\n |              path or resolves to something that is not an\n |              ``nn.Parameter``\n |  \n |  get_submodule(self, target: str) -> 'Module'\n |      Returns the submodule given by ``target`` if it exists,\n |      otherwise throws an error.\n |      \n |      For example, let's say you have an ``nn.Module`` ``A`` that\n |      looks like this:\n |      \n |      .. code-block:: text\n |      \n |          A(\n |              (net_b): Module(\n |                  (net_c): Module(\n |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n |                  )\n |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n |              )\n |          )\n |      \n |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n |      submodule ``net_b``, which itself has two submodules ``net_c``\n |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n |      \n |      To check whether or not we have the ``linear`` submodule, we\n |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n |      we have the ``conv`` submodule, we would call\n |      ``get_submodule(\"net_b.net_c.conv\")``.\n |      \n |      The runtime of ``get_submodule`` is bounded by the degree\n |      of module nesting in ``target``. A query against\n |      ``named_modules`` achieves the same result, but it is O(N) in\n |      the number of transitive modules. So, for a simple check to see\n |      if some submodule exists, ``get_submodule`` should always be\n |      used.\n |      \n |      Args:\n |          target: The fully-qualified string name of the submodule\n |              to look for. (See above example for how to specify a\n |              fully-qualified string.)\n |      \n |      Returns:\n |          torch.nn.Module: The submodule referenced by ``target``\n |      \n |      Raises:\n |          AttributeError: If the target string references an invalid\n |              path or resolves to something that is not an\n |              ``nn.Module``\n |  \n |  half(self: ~T) -> ~T\n |      Casts all floating point parameters and buffers to ``half`` datatype.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Returns:\n |          Module: self\n |  \n |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n |      Moves all model parameters and buffers to the IPU.\n |      \n |      This also makes associated parameters and buffers different objects. So\n |      it should be called before constructing optimizer if the module will\n |      live on IPU while being optimized.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Arguments:\n |          device (int, optional): if specified, all parameters will be\n |              copied to that device\n |      \n |      Returns:\n |          Module: self\n |  \n |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n |      Copies parameters and buffers from :attr:`state_dict` into\n |      this module and its descendants. If :attr:`strict` is ``True``, then\n |      the keys of :attr:`state_dict` must exactly match the keys returned\n |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n |      \n |      .. warning::\n |          If :attr:`assign` is ``True`` the optimizer must be created after\n |          the call to :attr:`load_state_dict`.\n |      \n |      Args:\n |          state_dict (dict): a dict containing parameters and\n |              persistent buffers.\n |          strict (bool, optional): whether to strictly enforce that the keys\n |              in :attr:`state_dict` match the keys returned by this module's\n |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n |          assign (bool, optional): whether to assign items in the state\n |              dictionary to their corresponding keys in the module instead\n |              of copying them inplace into the module's current parameters and buffers.\n |              When ``False``, the properties of the tensors in the current\n |              module are preserved while when ``True``, the properties of the\n |              Tensors in the state dict are preserved.\n |              Default: ``False``\n |      \n |      Returns:\n |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n |              * **missing_keys** is a list of str containing the missing keys\n |              * **unexpected_keys** is a list of str containing the unexpected keys\n |      \n |      Note:\n |          If a parameter or buffer is registered as ``None`` and its corresponding key\n |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n |          ``RuntimeError``.\n |  \n |  modules(self) -> Iterator[ForwardRef('Module')]\n |      Returns an iterator over all modules in the network.\n |      \n |      Yields:\n |          Module: a module in the network\n |      \n |      Note:\n |          Duplicate modules are returned only once. In the following\n |          example, ``l`` will be returned only once.\n |      \n |      Example::\n |      \n |          >>> l = nn.Linear(2, 2)\n |          >>> net = nn.Sequential(l, l)\n |          >>> for idx, m in enumerate(net.modules()):\n |          ...     print(idx, '->', m)\n |      \n |          0 -> Sequential(\n |            (0): Linear(in_features=2, out_features=2, bias=True)\n |            (1): Linear(in_features=2, out_features=2, bias=True)\n |          )\n |          1 -> Linear(in_features=2, out_features=2, bias=True)\n |  \n |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n |      Returns an iterator over module buffers, yielding both the\n |      name of the buffer as well as the buffer itself.\n |      \n |      Args:\n |          prefix (str): prefix to prepend to all buffer names.\n |          recurse (bool, optional): if True, then yields buffers of this module\n |              and all submodules. Otherwise, yields only buffers that\n |              are direct members of this module. Defaults to True.\n |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n |      \n |      Yields:\n |          (str, torch.Tensor): Tuple containing the name and buffer\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> for name, buf in self.named_buffers():\n |          >>>     if name in ['running_var']:\n |          >>>         print(buf.size())\n |  \n |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n |      Returns an iterator over immediate children modules, yielding both\n |      the name of the module as well as the module itself.\n |      \n |      Yields:\n |          (str, Module): Tuple containing a name and child module\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> for name, module in model.named_children():\n |          >>>     if name in ['conv4', 'conv5']:\n |          >>>         print(module)\n |  \n |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n |      Returns an iterator over all modules in the network, yielding\n |      both the name of the module as well as the module itself.\n |      \n |      Args:\n |          memo: a memo to store the set of modules already added to the result\n |          prefix: a prefix that will be added to the name of the module\n |          remove_duplicate: whether to remove the duplicated module instances in the result\n |              or not\n |      \n |      Yields:\n |          (str, Module): Tuple of name and module\n |      \n |      Note:\n |          Duplicate modules are returned only once. In the following\n |          example, ``l`` will be returned only once.\n |      \n |      Example::\n |      \n |          >>> l = nn.Linear(2, 2)\n |          >>> net = nn.Sequential(l, l)\n |          >>> for idx, m in enumerate(net.named_modules()):\n |          ...     print(idx, '->', m)\n |      \n |          0 -> ('', Sequential(\n |            (0): Linear(in_features=2, out_features=2, bias=True)\n |            (1): Linear(in_features=2, out_features=2, bias=True)\n |          ))\n |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n |  \n |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n |      Returns an iterator over module parameters, yielding both the\n |      name of the parameter as well as the parameter itself.\n |      \n |      Args:\n |          prefix (str): prefix to prepend to all parameter names.\n |          recurse (bool): if True, then yields parameters of this module\n |              and all submodules. Otherwise, yields only parameters that\n |              are direct members of this module.\n |          remove_duplicate (bool, optional): whether to remove the duplicated\n |              parameters in the result. Defaults to True.\n |      \n |      Yields:\n |          (str, Parameter): Tuple containing the name and parameter\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> for name, param in self.named_parameters():\n |          >>>     if name in ['bias']:\n |          >>>         print(param.size())\n |  \n |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n |      Returns an iterator over module parameters.\n |      \n |      This is typically passed to an optimizer.\n |      \n |      Args:\n |          recurse (bool): if True, then yields parameters of this module\n |              and all submodules. Otherwise, yields only parameters that\n |              are direct members of this module.\n |      \n |      Yields:\n |          Parameter: module parameter\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> for param in model.parameters():\n |          >>>     print(type(param), param.size())\n |          <class 'torch.Tensor'> (20L,)\n |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n |  \n |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n |      Registers a backward hook on the module.\n |      \n |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n |      the behavior of this function will change in future versions.\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n |      Adds a buffer to the module.\n |      \n |      This is typically used to register a buffer that should not to be\n |      considered a model parameter. For example, BatchNorm's ``running_mean``\n |      is not a parameter, but is part of the module's state. Buffers, by\n |      default, are persistent and will be saved alongside parameters. This\n |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n |      only difference between a persistent buffer and a non-persistent buffer\n |      is that the latter will not be a part of this module's\n |      :attr:`state_dict`.\n |      \n |      Buffers can be accessed as attributes using given names.\n |      \n |      Args:\n |          name (str): name of the buffer. The buffer can be accessed\n |              from this module using the given name\n |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n |              the buffer is **not** included in the module's :attr:`state_dict`.\n |          persistent (bool): whether the buffer is part of this module's\n |              :attr:`state_dict`.\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n |  \n |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n |      Registers a forward hook on the module.\n |      \n |      The hook will be called every time after :func:`forward` has computed an output.\n |      \n |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n |      the positional arguments given to the module. Keyword arguments won't be\n |      passed to the hooks and only to the ``forward``. The hook can modify the\n |      output. It can modify the input inplace but it will not have effect on\n |      forward since this is called after :func:`forward` is called. The hook\n |      should have the following signature::\n |      \n |          hook(module, args, output) -> None or modified output\n |      \n |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n |      ``kwargs`` given to the forward function and be expected to return the\n |      output possibly modified. The hook should have the following signature::\n |      \n |          hook(module, args, kwargs, output) -> None or modified output\n |      \n |      Args:\n |          hook (Callable): The user defined hook to be registered.\n |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n |              before all existing ``forward`` hooks on this\n |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n |              ``hook`` will be fired after all existing ``forward`` hooks on\n |              this :class:`torch.nn.modules.Module`. Note that global\n |              ``forward`` hooks registered with\n |              :func:`register_module_forward_hook` will fire before all hooks\n |              registered by this method.\n |              Default: ``False``\n |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n |              kwargs given to the forward function.\n |              Default: ``False``\n |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n |              whether an exception is raised while calling the Module.\n |              Default: ``False``\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n |      Registers a forward pre-hook on the module.\n |      \n |      The hook will be called every time before :func:`forward` is invoked.\n |      \n |      \n |      If ``with_kwargs`` is false or not specified, the input contains only\n |      the positional arguments given to the module. Keyword arguments won't be\n |      passed to the hooks and only to the ``forward``. The hook can modify the\n |      input. User can either return a tuple or a single modified value in the\n |      hook. We will wrap the value into a tuple if a single value is returned\n |      (unless that value is already a tuple). The hook should have the\n |      following signature::\n |      \n |          hook(module, args) -> None or modified input\n |      \n |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n |      kwargs given to the forward function. And if the hook modifies the\n |      input, both the args and kwargs should be returned. The hook should have\n |      the following signature::\n |      \n |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n |      \n |      Args:\n |          hook (Callable): The user defined hook to be registered.\n |          prepend (bool): If true, the provided ``hook`` will be fired before\n |              all existing ``forward_pre`` hooks on this\n |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n |              on this :class:`torch.nn.modules.Module`. Note that global\n |              ``forward_pre`` hooks registered with\n |              :func:`register_module_forward_pre_hook` will fire before all\n |              hooks registered by this method.\n |              Default: ``False``\n |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n |              given to the forward function.\n |              Default: ``False``\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n |      Registers a backward hook on the module.\n |      \n |      The hook will be called every time the gradients with respect to a module\n |      are computed, i.e. the hook will execute if and only if the gradients with\n |      respect to module outputs are computed. The hook should have the following\n |      signature::\n |      \n |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n |      \n |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n |      with respect to the inputs and outputs respectively. The hook should\n |      not modify its arguments, but it can optionally return a new gradient with\n |      respect to the input that will be used in place of :attr:`grad_input` in\n |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n |      as positional arguments and all kwarg arguments are ignored. Entries\n |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n |      arguments.\n |      \n |      For technical reasons, when this hook is applied to a Module, its forward function will\n |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n |      of each Tensor returned by the Module's forward function.\n |      \n |      .. warning ::\n |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n |          will raise an error.\n |      \n |      Args:\n |          hook (Callable): The user-defined hook to be registered.\n |          prepend (bool): If true, the provided ``hook`` will be fired before\n |              all existing ``backward`` hooks on this\n |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n |              ``hook`` will be fired after all existing ``backward`` hooks on\n |              this :class:`torch.nn.modules.Module`. Note that global\n |              ``backward`` hooks registered with\n |              :func:`register_module_full_backward_hook` will fire before\n |              all hooks registered by this method.\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n |      Registers a backward pre-hook on the module.\n |      \n |      The hook will be called every time the gradients for the module are computed.\n |      The hook should have the following signature::\n |      \n |          hook(module, grad_output) -> tuple[Tensor] or None\n |      \n |      The :attr:`grad_output` is a tuple. The hook should\n |      not modify its arguments, but it can optionally return a new gradient with\n |      respect to the output that will be used in place of :attr:`grad_output` in\n |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n |      all non-Tensor arguments.\n |      \n |      For technical reasons, when this hook is applied to a Module, its forward function will\n |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n |      of each Tensor returned by the Module's forward function.\n |      \n |      .. warning ::\n |          Modifying inputs inplace is not allowed when using backward hooks and\n |          will raise an error.\n |      \n |      Args:\n |          hook (Callable): The user-defined hook to be registered.\n |          prepend (bool): If true, the provided ``hook`` will be fired before\n |              all existing ``backward_pre`` hooks on this\n |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n |              on this :class:`torch.nn.modules.Module`. Note that global\n |              ``backward_pre`` hooks registered with\n |              :func:`register_module_full_backward_pre_hook` will fire before\n |              all hooks registered by this method.\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_load_state_dict_post_hook(self, hook)\n |      Registers a post hook to be run after module's ``load_state_dict``\n |      is called.\n |      \n |      It should have the following signature::\n |          hook(module, incompatible_keys) -> None\n |      \n |      The ``module`` argument is the current module that this hook is registered\n |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n |      is a ``list`` of ``str`` containing the missing keys and\n |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n |      \n |      The given incompatible_keys can be modified inplace if needed.\n |      \n |      Note that the checks performed when calling :func:`load_state_dict` with\n |      ``strict=True`` are affected by modifications the hook makes to\n |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n |      set of keys will result in an error being thrown when ``strict=True``, and\n |      clearing out both missing and unexpected keys will avoid an error.\n |      \n |      Returns:\n |          :class:`torch.utils.hooks.RemovableHandle`:\n |              a handle that can be used to remove the added hook by calling\n |              ``handle.remove()``\n |  \n |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n |      Alias for :func:`add_module`.\n |  \n |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n |      Adds a parameter to the module.\n |      \n |      The parameter can be accessed as an attribute using given name.\n |      \n |      Args:\n |          name (str): name of the parameter. The parameter can be accessed\n |              from this module using the given name\n |          param (Parameter or None): parameter to be added to the module. If\n |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n |              are ignored. If ``None``, the parameter is **not** included in the\n |              module's :attr:`state_dict`.\n |  \n |  register_state_dict_pre_hook(self, hook)\n |      These hooks will be called with arguments: ``self``, ``prefix``,\n |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n |      hooks can be used to perform pre-processing before the ``state_dict``\n |      call is made.\n |  \n |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n |      Change if autograd should record operations on parameters in this\n |      module.\n |      \n |      This method sets the parameters' :attr:`requires_grad` attributes\n |      in-place.\n |      \n |      This method is helpful for freezing part of the module for finetuning\n |      or training parts of a model individually (e.g., GAN training).\n |      \n |      See :ref:`locally-disable-grad-doc` for a comparison between\n |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n |      \n |      Args:\n |          requires_grad (bool): whether autograd should record operations on\n |                                parameters in this module. Default: ``True``.\n |      \n |      Returns:\n |          Module: self\n |  \n |  set_extra_state(self, state: Any)\n |      This function is called from :func:`load_state_dict` to handle any extra state\n |      found within the `state_dict`. Implement this function and a corresponding\n |      :func:`get_extra_state` for your module if you need to store extra state within its\n |      `state_dict`.\n |      \n |      Args:\n |          state (dict): Extra state from the `state_dict`\n |  \n |  share_memory(self: ~T) -> ~T\n |      See :meth:`torch.Tensor.share_memory_`\n |  \n |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n |      Returns a dictionary containing references to the whole state of the module.\n |      \n |      Both parameters and persistent buffers (e.g. running averages) are\n |      included. Keys are corresponding parameter and buffer names.\n |      Parameters and buffers set to ``None`` are not included.\n |      \n |      .. note::\n |          The returned object is a shallow copy. It contains references\n |          to the module's parameters and buffers.\n |      \n |      .. warning::\n |          Currently ``state_dict()`` also accepts positional arguments for\n |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n |          this is being deprecated and keyword arguments will be enforced in\n |          future releases.\n |      \n |      .. warning::\n |          Please avoid the use of argument ``destination`` as it is not\n |          designed for end-users.\n |      \n |      Args:\n |          destination (dict, optional): If provided, the state of module will\n |              be updated into the dict and the same object is returned.\n |              Otherwise, an ``OrderedDict`` will be created and returned.\n |              Default: ``None``.\n |          prefix (str, optional): a prefix added to parameter and buffer\n |              names to compose the keys in state_dict. Default: ``''``.\n |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n |              returned in the state dict are detached from autograd. If it's\n |              set to ``True``, detaching will not be performed.\n |              Default: ``False``.\n |      \n |      Returns:\n |          dict:\n |              a dictionary containing a whole state of the module\n |      \n |      Example::\n |      \n |          >>> # xdoctest: +SKIP(\"undefined vars\")\n |          >>> module.state_dict().keys()\n |          ['bias', 'weight']\n |  \n |  to(self, *args, **kwargs)\n |      Moves and/or casts the parameters and buffers.\n |      \n |      This can be called as\n |      \n |      .. function:: to(device=None, dtype=None, non_blocking=False)\n |         :noindex:\n |      \n |      .. function:: to(dtype, non_blocking=False)\n |         :noindex:\n |      \n |      .. function:: to(tensor, non_blocking=False)\n |         :noindex:\n |      \n |      .. function:: to(memory_format=torch.channels_last)\n |         :noindex:\n |      \n |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n |      (if given). The integral parameters and buffers will be moved\n |      :attr:`device`, if that is given, but with dtypes unchanged. When\n |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n |      with respect to the host if possible, e.g., moving CPU Tensors with\n |      pinned memory to CUDA devices.\n |      \n |      See below for examples.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Args:\n |          device (:class:`torch.device`): the desired device of the parameters\n |              and buffers in this module\n |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n |              the parameters and buffers in this module\n |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n |              dtype and device for all parameters and buffers in this module\n |          memory_format (:class:`torch.memory_format`): the desired memory\n |              format for 4D parameters and buffers in this module (keyword\n |              only argument)\n |      \n |      Returns:\n |          Module: self\n |      \n |      Examples::\n |      \n |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n |          >>> linear = nn.Linear(2, 2)\n |          >>> linear.weight\n |          Parameter containing:\n |          tensor([[ 0.1913, -0.3420],\n |                  [-0.5113, -0.2325]])\n |          >>> linear.to(torch.double)\n |          Linear(in_features=2, out_features=2, bias=True)\n |          >>> linear.weight\n |          Parameter containing:\n |          tensor([[ 0.1913, -0.3420],\n |                  [-0.5113, -0.2325]], dtype=torch.float64)\n |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n |          >>> gpu1 = torch.device(\"cuda:1\")\n |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n |          Linear(in_features=2, out_features=2, bias=True)\n |          >>> linear.weight\n |          Parameter containing:\n |          tensor([[ 0.1914, -0.3420],\n |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n |          >>> cpu = torch.device(\"cpu\")\n |          >>> linear.to(cpu)\n |          Linear(in_features=2, out_features=2, bias=True)\n |          >>> linear.weight\n |          Parameter containing:\n |          tensor([[ 0.1914, -0.3420],\n |                  [-0.5112, -0.2324]], dtype=torch.float16)\n |      \n |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n |          >>> linear.weight\n |          Parameter containing:\n |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n |          tensor([[0.6122+0.j, 0.1150+0.j],\n |                  [0.6122+0.j, 0.1150+0.j],\n |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n |  \n |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n |      Moves the parameters and buffers to the specified device without copying storage.\n |      \n |      Args:\n |          device (:class:`torch.device`): The desired device of the parameters\n |              and buffers in this module.\n |          recurse (bool): Whether parameters and buffers of submodules should\n |              be recursively moved to the specified device.\n |      \n |      Returns:\n |          Module: self\n |  \n |  train(self: ~T, mode: bool = True) -> ~T\n |      Sets the module in training mode.\n |      \n |      This has any effect only on certain modules. See documentations of\n |      particular modules for details of their behaviors in training/evaluation\n |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n |      etc.\n |      \n |      Args:\n |          mode (bool): whether to set training mode (``True``) or evaluation\n |                       mode (``False``). Default: ``True``.\n |      \n |      Returns:\n |          Module: self\n |  \n |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n |      Casts all parameters and buffers to :attr:`dst_type`.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Args:\n |          dst_type (type or string): the desired type\n |      \n |      Returns:\n |          Module: self\n |  \n |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n |      Moves all model parameters and buffers to the XPU.\n |      \n |      This also makes associated parameters and buffers different objects. So\n |      it should be called before constructing optimizer if the module will\n |      live on XPU while being optimized.\n |      \n |      .. note::\n |          This method modifies the module in-place.\n |      \n |      Arguments:\n |          device (int, optional): if specified, all parameters will be\n |              copied to that device\n |      \n |      Returns:\n |          Module: self\n |  \n |  zero_grad(self, set_to_none: bool = True) -> None\n |      Resets gradients of all model parameters. See similar function\n |      under :class:`torch.optim.Optimizer` for more context.\n |      \n |      Args:\n |          set_to_none (bool): instead of setting to zero, set the grads to None.\n |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from torch.nn.modules.module.Module:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from torch.nn.modules.module.Module:\n |  \n |  T_destination = ~T_destination\n |  \n |  call_super_init = False\n |  \n |  dump_patches = False\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-05-28T16:57:23.087725Z","iopub.execute_input":"2024-05-28T16:57:23.088347Z","iopub.status.idle":"2024-05-28T16:57:23.095807Z","shell.execute_reply.started":"2024-05-28T16:57:23.088313Z","shell.execute_reply":"2024-05-28T16:57:23.094843Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"('model', DetectionModel(\n  (model): Sequential(\n    (0): Conv(\n      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (1): Conv(\n      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (2): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (3): Conv(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (4): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (5): Conv(\n      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (6): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0-1): 2 x Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (7): Conv(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (8): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (9): SPPF(\n      (cv1): Conv(\n        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n    )\n    (10): Upsample(scale_factor=2.0, mode='nearest')\n    (11): Concat()\n    (12): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (13): Upsample(scale_factor=2.0, mode='nearest')\n    (14): Concat()\n    (15): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (16): Conv(\n      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (17): Concat()\n    (18): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (19): Conv(\n      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n      (act): SiLU(inplace=True)\n    )\n    (20): Concat()\n    (21): C2f(\n      (cv1): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (cv2): Conv(\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (m): ModuleList(\n        (0): Bottleneck(\n          (cv1): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (cv2): Conv(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n        )\n      )\n    )\n    (22): Detect(\n      (cv2): ModuleList(\n        (0): Sequential(\n          (0): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (0): Conv(\n            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (0): Conv(\n            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (cv3): ModuleList(\n        (0): Sequential(\n          (0): Conv(\n            (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (0): Conv(\n            (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (0): Conv(\n            (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (1): Conv(\n            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n            (act): SiLU(inplace=True)\n          )\n          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (dfl): DFL(\n        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n    )\n  )\n))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}